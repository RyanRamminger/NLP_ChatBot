{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLP Basics\n",
    "\n",
    "setting up training data. \n",
    "\n",
    "different patterns as inputs: \n",
    "\"Hi\" or \"How are you\" are tagged as GREETINGS\n",
    "\"Bye\" or \"See you later\" are tagged as GOODBYES\n",
    "\n",
    "we have to train our deep learning model, but we can't just use strings. \n",
    "We have to convert the strings to a vector that contains numbers. We will use \"bag of words\" concept. \n",
    "Bag of words places all inputed words into an array as SINGLE words\n",
    "\n",
    "    all words: \n",
    "        [\"Hi\", \"How\", \"are\", \"you\", \"bye\", \"see\", \"later\"]\n",
    "We will create an array with the same size or number of words in our array bag. If a word is used, we place a 1 in that spot and a 0 in the other places. \n",
    "\n",
    "           bag of words INPUTS(x)                           OUTPUTS(y)\n",
    "    \"Hi\"             --> [1, 0, 0, 0, 0, 0, 0]              0(greetings)\n",
    "    \"how are you?\"   --> [0, 1, 1, 1, 0, 0, 0] \n",
    "\n",
    "    \"Bye\"            --> [0, 0, 0, 0, 1, 0, 0]              1 (goodbyes)\n",
    "    \"See you later\"  --> [0, 0, 0, 1, 0, 1, 1]     \n",
    "\n",
    "we will be using a simple feed forward neural net with two hidden layers\n",
    "x (bag of words) as input  -->  number of patterns --> number of classes --> softmax --> OUTPUTS\n",
    "\n",
    "\n",
    "We will use TOKENIZATION to split the string into meaningful units (words, punctuation characters, numbers)\n",
    "\"what would you do with 1000000$?\"\n",
    "--> [\"what\", \"would\", \"you\", \"do\", \"with\", \"1000000\", \"$\", \"?\"] #returns an array\n",
    "\"aren't you happy with so much money?\"\n",
    "--> [\"are\", \"n't\", \"you\", \"happy\", \"with\", \"so\", \"much\", \"money\", \"?\"] #sometimes the token may separate aren't into two words\n",
    "\n",
    "STEMMING generates the root form of the words. crude heuristic that chops off the ends off of words.\n",
    "\"organize\", \"organizes\", \"organizing\"\n",
    "--> [\"organ\", \"organ\", \"organ\"]\n",
    "\"universe\", \"university\" #stemming may have us loose the meaning of the word sometimes...\n",
    "-->[\"univers\", \"univers\"]\n",
    "\n",
    "our NLP preprocessing pipeline:\n",
    "1) TOKENIZE our sentances\n",
    "    \"Is anyone there?\"  --> [\"Is\", \"anyone\", \"there\", \"?\"]\n",
    "2) LOWERCASE and STEM the words\n",
    "    [\"is\", \"anyon\", \"there\", \"?\"]\n",
    "3) EXCLUDE punctuation characters\n",
    "    [\"is\", \"anyon\", \"there\"]\n",
    "4) put in BAG OF words (INPUT X vector for training data)\n",
    "    X [0, 0, 0, 1, 0, 1, 0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will use a framework \"NLTK\" natural language toolkit\n",
    "https://www.nltk.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
